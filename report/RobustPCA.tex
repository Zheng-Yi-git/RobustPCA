\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[a4paper,centering,scale=0.8]{geometry}
\usepackage{indentfirst}
\setlength{\parindent}{2em} %2em代表首行缩进两个字符
\renewcommand{\baselinestretch}{1.5}
\begin{document}
\title{Robust Principal Component Analysis}
\date{\today}
\author{Yi Zheng, Yichuan Sun}
\maketitle
\tableofcontents     %（书上说要编译两次）
\newpage    
\section{Introduction}

Principal component analysis (PCA) is the process of computing the principal components and using them 
to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest, 
by which people can successfully reduce the dimension of the data. In short, PCA tries to find the best projection on a low-dimensional subspace.

Consider a common situation where we have a signal matrix $D$ containing the useful structural information as well as the noise, 
and we want to decompose it into the summation of two matrices, $A$ and $E$, such that
$$
D = A + E,
$$
where $A$ is the matrix of useful information and $E$ is the matrix of noise. Often, $A$ is low-rank (due to some intrinsic structure 
of the information, the columns or rows may be linearly dependent) and $E$ is sparse. When we applied PCA, we assume that the noises are 
Gaussian. But if there are some non-Gaussian noise or big outliers, this assumption may not be satisfied and PCA may have trouble working properly. 
Unfortunately, these kinds of gross errors are very
common in many applications, arising for example from corrupted data, sensor failures or corrupted
samples in repetitive measurement tasks in biology applications. 
Thus, Robust PCA was introduced to solve this problem, which is a promising way even in the presence of such gross but sparse corruptions. It 
combines the two popular heuristics of nuclear norm minimization (used to encourage
low-rankness) and $\ell_1$-norm minimization (used to encourage sparsity) and casts the problem of separating
a low-rank "data" component from a sparse "noise" component into a convex optimization
problem.

Over the past decade there has been an explosion in terms of the massive amounts of highdimensional
data in almost all fields of science and engineering, which is largely due to its success
in numerous application domains, ranging from bioinformatics,
statistics, and machine learning to image and video processing
in computer vision.
In such applications, researches today routinely deal with data that lie in thousands or even billions
of dimensions, with a number of samples sometimes of the same order of magnitude. Therefore, it makes sense to explore 
this method. In this project, we implemented two classic robust PCA algorithms, \textit{Inexact Augmented Lagrange Multiplier (IALM) Method} and 
\textit{Alternating Direction Method of Multipliers (ADMM)} to solve the problem of Robust PCA. Numerical results are attached.

\section{Restatement of the problem}
\section{Generation of test matrix}
The object of robust is to decompose a low rank matrix with sparse noise. 
To simulate the problem. We get the target matrix with the following steps.
\begin{itemize}
    \item Get an orthogonal matrix for the required $n$.
    \item Cut off the matrix to the required rank, get matrix of $n \times rank$
    \item Amplify each column with random numbers
    \item Multiply this matrix with its transpose, get the low rank matrix $n\times n$
    \item generate $S$ random numbers(unique), $S$ refers to the location of the noise
    \item Add random numbers to the location generated by the former step in the low rank matrix, get the final matrix.
\end{itemize}
\end{document}